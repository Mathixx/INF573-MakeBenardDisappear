{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-b/2022/mathias.perez/Desktop/INF573-MakeBenardDisappear/ikomia_env/lib64/python3.9/site-packages/numpy/core/getlimits.py:500: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/users/eleves-b/2022/mathias.perez/Desktop/INF573-MakeBenardDisappear/ikomia_env/lib64/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/users/eleves-b/2022/mathias.perez/Desktop/INF573-MakeBenardDisappear/ikomia_env/lib64/python3.9/site-packages/numpy/core/getlimits.py:500: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/users/eleves-b/2022/mathias.perez/Desktop/INF573-MakeBenardDisappear/ikomia_env/lib64/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary dependencies\n",
    "import cv2\n",
    "import numpy as np\n",
    "import unittest\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "\n",
    "# Import the necessary classes\n",
    "from detectors import YOLODetector\n",
    "\n",
    "from segmentors import YoloSegmentor\n",
    "\n",
    "from removers import BlurringRemover\n",
    "from removers import OpenCvInpaintingRemover\n",
    "from removers import LamaInpaintingRemover\n",
    "from removers import BetterLamaInpaintingRemover\n",
    "\n",
    "\n",
    "from benard_supressor import BenardSupressor\n",
    "\n",
    "# interactive notebook widgets\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Python>=3.10 is required, but Python==3.9.18 is currently installed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 16:23:28,644 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n",
      "/users/eleves-b/2022/mathias.perez/Desktop/INF573-MakeBenardDisappear/ikomia_env/lib64/python3.9/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "2024-11-07 16:23:29,375 - modelscope - INFO - initiate model from /users/eleves-b/2022/mathias.perez/.cache/modelscope/hub/damo/cv_fft_inpainting_lama\n",
      "2024-11-07 16:23:29,375 - modelscope - INFO - initiate model from location /users/eleves-b/2022/mathias.perez/.cache/modelscope/hub/damo/cv_fft_inpainting_lama.\n",
      "2024-11-07 16:23:29,376 - modelscope - INFO - initialize model from /users/eleves-b/2022/mathias.perez/.cache/modelscope/hub/damo/cv_fft_inpainting_lama\n",
      "2024-11-07 16:23:29,383 - modelscope - INFO - BaseInpaintingTrainingModule init called, predict_only is False\n",
      "2024-11-07 16:23:29,892 - modelscope - INFO - BaseInpaintingTrainingModule init done\n",
      "2024-11-07 16:23:29,892 - modelscope - INFO - loading pretrained model from /users/eleves-b/2022/mathias.perez/.cache/modelscope/hub/damo/cv_fft_inpainting_lama/pytorch_model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for net_encoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 16:23:30,072 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2024-11-07 16:23:30,073 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2024-11-07 16:23:30,073 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': '/users/eleves-b/2022/mathias.perez/.cache/modelscope/hub/damo/cv_fft_inpainting_lama'}. trying to build by task and model information.\n",
      "2024-11-07 16:23:30,073 - modelscope - WARNING - No preprocessor key ('FFTInpainting', 'image-inpainting') found in PREPROCESSOR_MAP, skip building preprocessor.\n",
      "2024-11-07 16:23:30,075 - modelscope - INFO - loading model from dir /users/eleves-b/2022/mathias.perez/.cache/modelscope/hub/damo/cv_fft_inpainting_lama\n",
      "2024-11-07 16:23:30,075 - modelscope - INFO - BaseInpaintingTrainingModule init called, predict_only is True\n",
      "2024-11-07 16:23:30,279 - modelscope - INFO - BaseInpaintingTrainingModule init done\n",
      "2024-11-07 16:23:30,280 - modelscope - INFO - loading pretrained model from /users/eleves-b/2022/mathias.perez/.cache/modelscope/hub/damo/cv_fft_inpainting_lama/pytorch_model.pt\n",
      "2024-11-07 16:23:30,428 - modelscope - INFO - loading model done, refinement is set to True\n"
     ]
    }
   ],
   "source": [
    "# Let's test the perfomance of a Benard Supressor on the test images\n",
    "\n",
    "detector = YOLODetector(device='cuda')\n",
    "segmentor = YoloSegmentor(device='cuda')\n",
    "remover = BetterLamaInpaintingRemover()\n",
    "\n",
    "benard_supressor = BenardSupressor(detector, segmentor, remover)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's refine our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_path = \"_test_data/video/red_cap_video.mp4\"\n",
    "video_capture = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "print(f\"Processing video: {input_video_path}\")\n",
    "    \n",
    "# Get video properties\n",
    "original_frame_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "original_frame_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(video_capture.get(cv2.CAP_PROP_FPS))\n",
    "print(f\"Original video properties: {original_frame_width}x{original_frame_height} @ {fps} FPS\")\n",
    "\n",
    "# Initialize variables\n",
    "frame_count = 0\n",
    "    \n",
    "# Process each frame\n",
    "while video_capture.isOpened():\n",
    "    print(f\"Processing frame {frame_count}\")\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break  # End of video stream\n",
    "        \n",
    "    # Process the frame with the process_image method\n",
    "    processed_frame = self.process_image(frame)\n",
    "\n",
    "    # Convert to uint8 if not already\n",
    "    if processed_frame.dtype != 'uint8':\n",
    "        print(\"Converting to uint8\")\n",
    "        processed_frame = processed_frame.astype('uint8')\n",
    "\n",
    "    # Check for and handle RGBA\n",
    "    if processed_frame.ndim == 3 and processed_frame.shape[2] == 4:\n",
    "        print(\"Converting RGBA to RGB\")\n",
    "        processed_frame = cv2.cvtColor(processed_frame, cv2.COLOR_RGBA2RGB)\n",
    "\n",
    "    # Check if a pixel has value > 255\n",
    "    if (processed_frame > 255).any():\n",
    "        print(\"Clipping values to 255\")\n",
    "        processed_frame = np.clip(processed_frame, 0, 255)\n",
    "            \n",
    "\n",
    "    if frame_count % 10 == 0:\n",
    "        # save the frame\n",
    "        cv2.imwrite(f\"frame_{frame_count}.png\", processed_frame)\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count > 100:\n",
    "        break  # Limit processing to the first 100 frames as specified\n",
    "\n",
    "# Release resources\n",
    "video_capture.release()\n",
    "if video_writer is not None:\n",
    "    video_writer.release()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ikomia_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
